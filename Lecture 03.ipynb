{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3\n",
    "# Extremal bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of hashing, take 2\n",
    "\n",
    "Suppose we have a hash table of size $n$, where we have inserted $n$ items into the $n$ locations at random. Recall that in order to search for an item $x$, we compute $h(x)$ and look at all of the items in location $h(x)$. This is commonly known as a balls-in-bins problem, and we will refer to the items as balls and the hash table locations as bins.\n",
    "\n",
    "We can ask a number of questions:\n",
    "\n",
    "- What is the average size of a bin?\n",
    "- What is the expected value of the size of a bin?\n",
    "- Can you say something of the form: bin $i$ has size at most $x$ with probability at most $p$?\n",
    "- Can you say something of the form: all bins have size at most $x$ with probability at most $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average\n",
    "\n",
    "We will be using the language of probability. Let $b_i$ denote the number of balls in bin $i$. The variable $b_i$ is not fixed, we use $Pr[b_i=x]$ to indicate the probability that after throwing $n$ balls into $n$ bins, that bin $i$ has size $x$. For example, $Pr[b_i=0]=(1-\\frac{1}{n})^n$, which as we learned last class, converges to $\\frac{1}{e}=37\\%$.\n",
    "\n",
    "The first question \"What is the average size of a bin\", is easy. Using the formula for average, and using the fact that the total number of balls is $n$:\n",
    "\n",
    "$$ \\text{Average bin size}= \\frac{1}{n}\\sum_{i=1}^n b_i =\\frac{1}{n}\\cdot n =1 $$\n",
    "\n",
    "This statement has nothing to do with probability. No matter how you put the balls into bins there is an average size of 1. The average size does not distinguish between one ball in each bin and all the balls in one bin. As we care about this difference, this shows that average is the wrong measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected value\n",
    "\n",
    "The second question is \"What is the expected value of the size of a bin?\" For this we need the definition of expected value:\n",
    "\n",
    "$$ E[X] = \\sum_j j Pr[X=j]$$ \n",
    "\n",
    "So with this definition in hand:\n",
    "\n",
    "$$ E[b_i] = \\sum_{j=0}^n j Pr[b_i=j] $$\n",
    "\n",
    "What is $Pr[b_i=j]$? This is ${n \\choose j}(1-\\frac{1}{n})^j(\\frac{1}{n})^{n-j}$. But there is a better way.\n",
    "\n",
    "The trick is to use something known as linearity of expectation, which says:\n",
    "\n",
    "$$ E[X+Y]=E[X]+E[Y]$$\n",
    "\n",
    "But $E[b_i]$ is not a sum. Well, in fact it is. Consider the following definition:\n",
    "\n",
    "$$ b_{i,j} = \\begin{cases} 1 & \\text{if ball $j$ lands in bin $i$}\\\\\n",
    "0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "The random variable $b_{i,j}$ is known as an *indicator random variable*, which simply means it is either 0 or 1 depending on whether some event occurs. What is $E[b_{i,j}]$? Well this is just the probability that the event *ball $j$ lands in bin $i$* occurs, which is $\\frac{1}{n}$.\n",
    "\n",
    "\n",
    "Then we observe that $b_i= \\sum_{j=1}^n b_{i,j}$. Using this we can now compute the expected value easily:\n",
    "\n",
    "$$\\begin{align*} E[b_i] \n",
    "&= E\\left[ \\sum_{j=1}^n b_{i,j} \\right] \n",
    "\\\\\n",
    "&= \\sum_{j=1}^n E[b_{i,j}]\n",
    "\\\\\n",
    "&= \\sum_{j=1}^n \\frac{1}{n}\n",
    "\\\\\n",
    "&= n\\cdot \\frac{1}{n}\n",
    "\\\\\n",
    "&= 1\n",
    "\\end{align*}$$\n",
    "\n",
    "Once again we get 1. And once again this is fairly meaningless, as this does not say much about the distribution. For example if someone picked a random bin, and then threw all the balls into that bin, the expected bin size for any bin is 1, yet the distribution is horribly skewed. In particular, the expected value alone does not say anything about the third question \"Can you say something of the form: bin $i$ has size at most $x$ with probability at most $p$?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov\n",
    "\n",
    "To try to bound things better, we will use something known as Markov's inequality. To motivate it, suppose I have 100 students, and the average mark is 4. Then it is the case that less than half of the students can have marks above 8, and at most a quarter of students can have marks above 16. This simply follows by contradiction, and vitally uses the fact that marks are not negative, otherwise you can't conclude anything.\n",
    "\n",
    "**Markov's ineqality:** Given random value $X$ that is always nonnegative:\n",
    "$$ Pr[X \\geq a ] \\leq \\frac{E[X]}{a}$$\n",
    "\n",
    "The proof follows directly by contradiction and the definition of expected value.\n",
    "\n",
    "Now, how does this apply to us? It allows us to give an answer to the third question. Expected value alone does not, but since the number of balls in bin $i$, $b_i$ is never negative, we can use Markov and our knowledge that $E[b_i]=1$:\n",
    "\n",
    "$$Pr[b_i\\geq x] \\leq \\frac{E[b_i]}{x} = \\frac{1}{x}$$\n",
    "\n",
    "So this is more meaningful. We can say that a bin has at least 100 items less than 1% of the time, thus it has less than 100 items at least 99% of this time.\n",
    "\n",
    "But, this bound is far from tight.\n",
    "\n",
    "Usually in a probability course you next learn Chebyshev's inequality, which is better than Markov when you know the variance of your random variable. Instead we will go straight to Chernoff bounds, which we will use several times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chernoff bounds\n",
    "\n",
    "Chernoff bounds provide excellent bounds on showing what the chance is that the outcome is close to the expected value. However, Chernoff bounds only work to bound a random variable that is the sum of independent random variables. \n",
    "\n",
    "Formally $X$ and $Y$ are independent if  for all $x,y$: $P[X=x \\text{ and } Y=y]=Pr[X=x]\\cdot Pr[Y=y]$. For example, $b_{i,j}$  and $b_{i,j'}$, $j \\not = j'$ are independent, this can be verified by looking at the probabilities of the four different outcomes of these two events, but informally whether ball $j$ ends up in bin $i$ has no effect on whether ball $j'$ ends up in bin $i$. On the other hand $b_{i,j}$ and $b_{i',j}$, $i \\not = i'$ are not independent! If the $j$th ball is in bin $i$ then it is certainly not in bin $i'$.\n",
    "\n",
    "There are many formulations of the statement of Chernoff bounds. One of the most usefull is\n",
    "\n",
    "\n",
    "**Chernoff bounds:** Let $X_1, X_2, \\ldots X_k$ be **independent** events with outcomes in the range $[0,1]$ and let $X=\\sum_{i=1}^k X_i$. Then:\n",
    "$$ \n",
    "\\begin{align*}\n",
    "Pr[X \\leq (1-\\delta) E[X] ] & \n",
    "\\leq\n",
    "\\left( \\frac{e^{-\\delta}}{(1-\\delta)^{1-\\delta}} \\right)^{E[X]}\n",
    "\\leq e^{-\\frac{\\delta^2E[X]}{2}}\n",
    "& \\text{For any $\\delta, 0\\leq \\delta \\leq 1$}\n",
    "\\\\\n",
    "Pr[X \\geq (1+\\delta) E[X] ] &\n",
    "\\leq \\left( \\frac{e^{\\delta}}{(1+\\delta)^{1+\\delta}} \\right)^{E[X]}\n",
    "\\leq e^{-\\frac{\\delta^2E[X]}{2+\\delta}}\n",
    "& \\text{For any $\\delta\\geq 0$}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So, how does this apply to our hash tables? Well, we defined $b_i= \\sum_{j=1}^n b_{i,j}$, and we know the $b_{i,j}$'s are independent for different $j$'s. We also know $E[b_i]=1$. This gives us all of the needed ingredients. We will use the second formula, as we want to limit the chance of being above the expected value by too much. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr[b_i \\geq (1+\\delta) E[b_i]] & \\leq e^{-\\frac{\\delta^2E[X]}{2+\\delta}}\n",
    "\\\\\n",
    "Pr[b_i \\geq (1+\\delta)] & \\leq e^{-\\frac{\\delta^2}{2+\\delta}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So, once again, we ask what is the chance that $b_i\\geq 100$? Markov bounded this at at most $1%$. To use Chernoff for this, set $\\delta=99$ and we get\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr[b_i \\geq 100 ]&\\leq e^{-\\frac{99^2}{2+99}} \n",
    "\\\\\n",
    "& = \\frac{1}{e^{\\frac{9801}{11}}}\n",
    "\\\\\n",
    "& \\approx 0.000000000000000000000000000000000000000000718\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So while Markov bounded 100 balls in a bin as a 1-in-100 event, small, but not impossible, Chernov says this will happen with a tiny probability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With high probability\n",
    "\n",
    "If an event happens with probability $1-O(\\frac{1}{n^c})$, for some $c$ this is said to be with *polynomially high probability* (whp) Such an event has a chance of happening that goes to 1 as $n$ grows. This is stronger than saying something about a certain fixed percentage, such as 1%. \n",
    "\n",
    "(*Note that with high probability means any bound that tends to 1 as $n$ grows, but in practice this term is often used to mean that it goes with a polynomial and not something like $1-\\frac{1}{\\log \\log n}$*)\n",
    "\n",
    "Chernoff bounds are often used to make whp bounds. As we want to turn something like $e^{-\\delta}$ into a $n^{-c}$ it is natural to try setting $\\delta$ to a logarithm, since $e^{-c \\ln n}=n^c$.\n",
    "\n",
    "We can ask what is the chance that $b_i \\geq 1+\\alpha \\ln n $. Setting $\\delta= \\alpha \\ln n$, for some $\\alpha \\geq 1$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr[b_i \\geq (1+\\delta)] & \\leq e^{-\\frac{\\delta^2}{2+\\delta}}\n",
    "\\\\\n",
    "Pr[b_i \\geq 1+\\alpha \\ln n] & \\leq e^{-\\frac{\\delta^2}{2+\\delta}}\n",
    "\\\\\n",
    "&= e^{-\\frac{\\alpha^2 \\ln^2 n}{2+\\alpha \\ln n}}\n",
    "\\\\\n",
    "&< e^{-\\frac{\\alpha^2 \\ln^2 n}{\\alpha \\ln n}}\n",
    "\\\\\n",
    "&= e^{-{\\alpha \\ln n}}\n",
    "\\\\\n",
    "&= \\frac{1}{n^{\\alpha}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "So we can conclude that $b_i$ is at least $1+2 \\ln n$ with probability at most $\\frac{1}{n^2}$, and at least $1+3 \\ln n$ with probability at most $\\frac{1}{n^3}$.\n",
    "This implies $b_i$ is less than $1+2 \\ln n$ with probability at least $1-\\frac{1}{n^2}$.\n",
    "This is often times stated $b_i$ is $O(\\ln n)$ whp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union bounds\n",
    "\n",
    "The Union bound, also known a Boole'e inequality is a very loose bound on multiple event happening: \n",
    "\n",
    "$$P[X=x\\text{ or }Y=y] \\leq P[X=x] + P[Y=y]$$\n",
    "\n",
    "It often times gives useless bounds. For example, it says that if you flip 4 coins, the chance they are all heads is\n",
    "\n",
    "$$P[\\text{4 coins all heads}] \\leq 4 P[\\text{one coin heads}] = 4\\cdot\\frac{1}{2} =2$$\n",
    "\n",
    "Saying a probability is less than two is not saying anything!\n",
    "\n",
    "But, sometimes a union bound is exactly what is needed. Its main advantage is that it always works, you don't need to assume independence or anything else.\n",
    "\n",
    "We will use this to solve question 4: *Can you say something of the form: all bins have size at most $x$ with probability at most $p$?* \n",
    "\n",
    "We know that a single bin has size at most $1+2 \\ln n$ with probability $\\frac{1}{n^2}$. What about the probability that *all* bins have size at most $1+2 \\ln n$? This is asking $Pr[\\text{for all $i$ in $1..n$}, b_i \\leq 1+2 \\ln n]$. Using the union bound:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr[\\text{for all $i$ in $1..n$}, b_i \\leq 1+2 \\ln n] \n",
    "& \\leq  \\sum_{i \\in 1..n} Pr[b_i \\leq 1+2 \\ln n]\n",
    "\\\\\n",
    "& \\leq  \\sum_{i \\in 1..n} \\frac{1}{n^2}\n",
    "\\\\\n",
    "& \\leq  n \\cdot \\frac{1}{n^2}\n",
    "\\\\\n",
    "& \\leq   \\frac{1}{n}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus not only is one bin have size $O(\\log n)$ whp, but all bins do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate median finding\n",
    "\n",
    "## Mean and median\n",
    "\n",
    "Recall that the *median* of a collection of numbers is the middle number in sorted order. The *mean* is the average.  The median is more robust, as a single bad value can change the mean by an arbitrary amount but not the median. Note that the median generalizes the idea of order statistic, and everything here can also be used for any oder statistic or to compute quantiles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 99, 32, 78, 2, 4, 6, 8]  mean:  25.88888888888889  median  6\n",
      "[10000, 3, 99, 32, 78, 2, 4, 6, 8]  mean:  1136.888888888889  median  8\n"
     ]
    }
   ],
   "source": [
    "# Returns the mean of a list\n",
    "def meanOfList(A):\n",
    "    return sum(A)/len(A) \n",
    "# Returns the median of a list\n",
    "def medianOfList(A):\n",
    "    Asorted=sorted(A)\n",
    "    return Asorted[len(A)//2]\n",
    "\n",
    "A=[1,3,99,32,78,2,4,6,8]\n",
    "print(A,\" mean: \",meanOfList(A),\" median \",medianOfList(A))\n",
    "A[0]=10000\n",
    "print(A,\" mean: \",meanOfList(A),\" median \",medianOfList(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, mean runs in time $O(n)$ while median uses sorting and runs in time $O(n \\log n)$. It is possible to compute the median in linear time. The [statistics](https://docs.python.org/3/library/statistics.html) package has efficient code for mean and median.\n",
    "\n",
    "Can we do better than linear time? Meaning can we compute the mean or the median without looking at all the data? The answer is no. For the mean we can no even approximate it without looking at all the values, as the one value we miss could change the mean in an unrestricted way.\n",
    "\n",
    "However, there is hope for the median. Let us define formally what a $\\epsilon$-approximate median is:\n",
    "\n",
    "Given a list of size $n$, $x$ is a $\\epsilon$-approximate median is any value from $\\frac{(1-\\epsilon)n}{2}$nd to the $\\frac{(1+\\epsilon)n}{2}$rd items in the list.\n",
    "\n",
    "So a $50\\%$-approximate median would be an item that would be in the middle half of the list, if sorted.\n",
    "\n",
    "Here is a very simple algorithm to return a $50\\%$-approximate median that works 50% of the time: pick a random element and return it. This only looks at one element.\n",
    "\n",
    "But the 50% failure probability is quite high. Can we design a method that looks at more than 1, but less than all of the items and has a lower failure probability?\n",
    "\n",
    "A simple idea is this: pick $k$ items at random and return the median of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approx median with k= 1 : 600625\n",
      "Approx median with k= 11 : 62500\n",
      "Approx median with k= 21 : 291600\n",
      "Approx median with k= 31 : 291600\n",
      "Approx median with k= 41 : 97344\n",
      "Approx median with k= 51 : 123201\n",
      "Approx median with k= 61 : 225625\n",
      "Approx median with k= 71 : 257049\n",
      "Approx median with k= 81 : 272484\n",
      "Approx median with k= 91 : 223729\n",
      "Real median 245025\n",
      "Real mean 330693.8944\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "def approxMedianOfList(A,k):\n",
    "    kRandom=random.choices(A,k=k)\n",
    "    kRandom.sort()\n",
    "    return kRandom[len(kRandom)//2]\n",
    "\n",
    "A=[random.randrange(1,1000)**2 for i in range(10000)]\n",
    "for k in range(1,100,10):\n",
    "    print(\"Approx median with k=\",k,\":\",approxMedianOfList(A,k))\n",
    "print(\"Real median\",medianOfList(A))\n",
    "print(\"Real mean\",meanOfList(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so how good is this? Once again we turn to Chernoff. Let us first bound the probability that the approximate median is too high.\n",
    "\n",
    "Let $X_i$ be the indicator random variable that is 1 iff the $i$th random value is not an $\\epsilon$-approximate median because it is too high, call this a high sample. We know $E[X_i]=\\frac{1-\\epsilon}{2}$. \n",
    "\n",
    "We know the median-of-medians is to high if at least half of the samples are high. Let $X$ be the total number of high samples, $\\sum_{i=1}^k X_i$. We want to bound\n",
    "\n",
    "$$ Pr[X \\geq \\frac{1}{2}k] $$\n",
    "\n",
    "Chernoff wants to see something like\n",
    "\n",
    "$$Pr[X \\geq (1+\\delta) E[X] ]$$\n",
    "\n",
    "As we know $E[x]= k \\cdot \\frac{1-\\epsilon}{2}$, we solve for $\\delta$:\n",
    "\n",
    "$$(1+\\delta)\\left( k \\cdot \\frac{1-\\epsilon}{2}\\right) = \\frac{1}{2}k\n",
    "$$\n",
    "\n",
    "Thus $\\delta= \\frac{1}{1-\\epsilon}-1$, which is at most 1. Thus we can use Chernoff as follows:\n",
    "\n",
    "$\\begin{align*}\n",
    "Pr[X \\geq (1+\\delta) E[X] ] &\\leq e^{-\\frac{\\delta^2E[X]}{2+\\delta}}\n",
    "\\\\\n",
    "&\\leq e^{-\\frac{\\delta^2E[X]}{3}}\n",
    "\\\\\n",
    "&\\leq e^{-\\frac{\\left(\\frac{1}{1-\\epsilon}-1\\right)^2 k \\cdot \\frac{1-\\epsilon}{2}}{3}}\n",
    "\\\\\n",
    "&= e^{ -k \\frac{\\epsilon^2}{6-6\\epsilon} }\n",
    "\\\\\n",
    "&\\leq  e^{ -\\frac{k\\epsilon^2}{6}}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "Remember, this was the chance of failure because there were to many samples that were too large. The chance of failure because there were too many samples that are too small is the same. Thus, by the union bound, the total chance of failure is at most $ 2  e^{ -\\frac{k\\epsilon^2}{6}}$.\n",
    "\n",
    "How should be choose $k$ to get a certain failure rate, call it $\\gamma$, for a $\\epsilon$-approximate median? Solve:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\gamma &= 2  e^{ -\\frac{k\\epsilon^2}{6}} \\\\\n",
    "\\gamma &= 2  e^{ -\\frac{k\\epsilon^2}{6}} \\\\ \n",
    "\\ln \\frac{\\gamma}{2} &= -\\frac{k\\epsilon^2}{6}\\\\\n",
    "\\frac{6}{\\epsilon^2} \\ln \\frac{\\gamma}{2}  &= - k  \\\\\n",
    "\\frac{6}{\\epsilon^2} \\ln \\frac{2}{\\gamma}  &=  k  \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "So, if we wanted to get a 10%-approximate median with a failure rate of $0.01\\%$, we set $\\epsilon=0.1$ and $\\gamma=0.0001$ which gives $\\gamma=5942$. Around 6000 may seem like a lot, but when you have millions or billions of data items, this is a real improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Succinct hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In normal hashing, there is often a waste of space linear in the number of items stored. For example, to store $n$ 64-bit numbers will take $cn$ 64=bit words, with $c$ being some constant greater than 1, which will depend on the language and how you code it.\n",
    "\n",
    "Here we will show how you can store a hash table using $n(1+\\frac{1}{\\log n})$ words of space, a quantity that approaches $1n$ in the limit. The idea is simple, by having each bucket have a fixed size, the entire table can be stored using a two-dimensional array with no other information needed, using python's array.array class (or just normal arrays in most other languages). \n",
    "\n",
    "As we saw earlier, if we use $n$ buckets, the size of the buckets needs to be $\\Theta(\\log n)$ to ensure that all the data that hashes into the bucket fits. Thus we would need a data structure of size $n \\cdot \\Theta(\\log n) = \\Theta(n \\log n)$. But such a structure would be huge, much worse in size than a standard hash table.\n",
    "\n",
    "However, by reducing the number of buckets and increasing their size, the math takes a surprising turn for the better. If we use $\\frac{n}{\\ln^4 n}$ buckets, and each bucket is size $\\ln^4 n + \\ln^3 n$, the total size is $\\frac{n}{\\ln^4 n}\\cdot (\\ln^4 n + \\ln^3 n) = n\\left(1+ \\frac{1}{\\ln n}\\right)$ with high probability. This is done with Chernoff/Union bounds in the next box below.\n",
    "\n",
    "The disadvantage of this method is that you now need $O(\\log^4 n)$ time to search each bucket instead of constant expected. This can be overcome with further effort, which I will not describe here but you can find in:\n",
    "\n",
    "https://epubs.siam.org/doi/epdf/10.1137/1.9781611977936.33\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are hashing $n$ values into $m$ buckets, where $m=\\frac{n}{\\ln^4 n}$\n",
    "\n",
    "Using the notation from above, we have $E[b_{i,j}] = \\frac{\\ln^4 n}{n}$ and $E[b_i] =  \\ln^4 n$.\n",
    "\n",
    "We would like to show that the $Pr[b_i \\geq \\ln^4 n + \\ln^3 n] \\leq \\frac{1}{n^2}$. \n",
    "\n",
    "We do this using one of the forms of Chernoff bounds:\n",
    "\n",
    "$$ Pr[X \\geq (1+\\delta)E[X]]\\leq \\left( \\frac{e^\\delta}{(1+\\delta)^{(1+\\delta)}} \\right)^{E[X]}$$\n",
    "\n",
    "\n",
    "Setting $E[X]=\\ln^4 n$ and $\\delta=\\frac{1}{\\ln n}$ so that $(1+\\delta)E[X]= \\ln^4 n + \\ln^3 n$ gives:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Pr[b_i \\geq \\ln^4 n + \\ln^3 n] \\leq \\frac{1}{n^2}\n",
    "&=\n",
    "\\left( \\frac{e^{\\frac{1}{\\ln n}}}{\\left(1+{\\frac{1}{\\ln n}}\\right)^{(1+{\\frac{1}{\\ln n}})}} \\right)^{\\ln^4 n}\n",
    "\\\\&=\n",
    " \\frac{e^{\\ln^3 n}}{\\left(1+{\\frac{1}{\\ln n}}\\right)^{\\ln^4n+\\ln^3 n}}\n",
    "\\\\\n",
    "&=  \\frac{e^{\\ln^3 n}}{\\left( \\left(1+{\\frac{1}{\\ln n}}\\right)^{\\ln n}\\right)^{\\ln^3n+\\ln^2 n}}\n",
    "\\\\ & \\approx \\frac{e^{\\ln^3 n}}{e^{\\ln^3n+\\ln^2 n}}\n",
    "\\\\ & =  \\frac{1}{e^{\\ln^2 n}}\n",
    "\\\\ & =  \\frac{1}{n^2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Thus using the union bound we can say that *all* $b_i$ are at most $\\ln^4 n + \\ln^3 n$ with probability $\\frac{1}{n}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
